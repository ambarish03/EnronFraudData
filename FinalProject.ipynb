{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I looked at the Enron fraud data which is publicly available here: https://www.cs.cmu.edu/~./enron/\n",
    "The objective of this study was to develop a predictive model to identify the persons-of-interest based on available data. The dataset included financial data for each person and detailed and summary information for emails exchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis was performed using python's sklearn package and the various classifiers included in the package. As a first step, I imported the packages necessary during course of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import scoreatpercentile\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cross_validation\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, I changed the current working directory to load existing user-defined functions that will ease the process of reading the source data without implementing the functionality once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/ambarishbanerjee/Downloads/ud120-projects/tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "os.chdir(\"/Users/ambarishbanerjee/Downloads/ud120-projects/final_project/\")\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we start by reading the income and summary of emails exchanged for each person in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary','deferral_payments',\\\n",
    "                 'total_payments','exercised_stock_options',\\\n",
    "                'from_messages','from_this_person_to_poi',\\\n",
    "                'bonus','restricted_stock','shared_receipt_with_poi',\\\n",
    "                 'to_messages','from_poi_to_this_person',\\\n",
    "                'restricted_stock_deferred','total_stock_value',\\\n",
    "                'expenses','loan_advances',\\\n",
    "                'other','director_fees','deferred_income',\\\n",
    "                'long_term_incentive']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step to our analysis, we begin by looking at the dataset. To that effect, considering the dataset comprises of about 150 records, I looked at the individual entries and noticed that there were two entries that did not appear to be any specific person's information. The first of this was the record identified as \"Total\" which is the gross total for all the people in our dataset. The second referred as \"Travel Agency in the Park\" could have been any agency contracted by Enron and hence the decision was to drop this record as well. I also observed that all fields for \"Eugene Lockhart\" contained NaN and hence the decision to drop this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 2: Remove outliers\n",
    "my_dataset = data_dict.copy()\n",
    "\n",
    "outlier_list = [\"TOTAL\",\"THE TRAVEL AGENCY IN THE PARK\",\"LOCKHART EUGENE E\"]\n",
    "\n",
    "for record in outlier_list:\n",
    "    if record in data_dict:\n",
    "        data_dict.pop(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the corrections made to the dataset, it was observed that some of the values did not tally with what was provided in the enron61702insiderpay.pdf for two specific people - Sanjay Bhatnagar and Robert Belfer. Therefore the financial information for these tworecords were manually updated in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for record in data_dict:\n",
    "    if record == \"BELFER ROBERT\":\n",
    "        data_dict[record]['salary'] = 'NaN'\n",
    "        data_dict[record]['bonus'] = 'NaN'\n",
    "        data_dict[record]['long_term_incentive'] = 'NaN'\n",
    "        data_dict[record]['deferred_income'] = -102500\n",
    "        data_dict[record]['deferral_payments'] = 'NaN'\n",
    "        data_dict[record]['loan_advances'] = 'NaN'\n",
    "        data_dict[record]['other'] = 'NaN'\n",
    "        data_dict[record]['expenses'] = 3285\n",
    "        data_dict[record]['director_fees'] = 102500\n",
    "        data_dict[record]['total_payments'] = 3285\n",
    "        data_dict[record]['exercised_stock_options'] = 'NaN'\n",
    "        data_dict[record]['restricted_stock'] = 44093\n",
    "        data_dict[record]['restricted_stock_deferred'] = -44093\n",
    "        data_dict[record]['total_stock_value'] = 'NaN'\n",
    "        #print data_dict[record]\n",
    "    if record == \"BHATNAGAR SANJAY\":\n",
    "        data_dict[record]['salary'] = 'NaN'\n",
    "        data_dict[record]['bonus'] = 'NaN'\n",
    "        data_dict[record]['long_term_incentive'] = 'NaN'\n",
    "        data_dict[record]['deferred_income'] = 'NaN'\n",
    "        data_dict[record]['deferral_payments'] = 'NaN'\n",
    "        data_dict[record]['loan_advances'] = 'NaN'\n",
    "        data_dict[record]['other'] = 'NaN'\n",
    "        data_dict[record]['expenses'] = 137864\n",
    "        data_dict[record]['director_fees'] = 'NaN'\n",
    "        data_dict[record]['total_payments'] = 137864\n",
    "        data_dict[record]['exercised_stock_options'] = 15456290\n",
    "        data_dict[record]['restricted_stock'] = 2604490\n",
    "        data_dict[record]['restricted_stock_deferred'] = -2604490\n",
    "        data_dict[record]['total_stock_value'] = 15456290\n",
    "        #print data_dict[record]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, I created a list containing a mapping between the email address and the person-of-interest variable with the intent of looking up the \"poi\" corresponding to an email while analyzing the Enron corpus. Following this, I added a synthtic variable called person_id with the intent of using the person_id as a durable key in lieu of the email address. To that end, I also stored the three variables - email_address, poi, and person_id in a tuple to lookup any of the other two values provided the third one is known. It should be also noted that there were a lot of records for whom the email address were not known but fortunately all such records corresponded to non-poi-s in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "### Store to data_dict for easy export below.\n",
    "email_list = []\n",
    "       \n",
    "person_id = 0\n",
    "for record in data_dict:\n",
    "    person_id += 1\n",
    "    data_dict[record]['person_id'] = person_id\n",
    "    \n",
    "email_lookup_tup = ()\n",
    "for record in data_dict:\n",
    "    email_list.append([data_dict[record]['email_address'],data_dict[record]['poi'],data_dict[record]['person_id']])\n",
    "    email_lookup_tup = email_lookup_tup+((data_dict[record]['person_id'],\\\n",
    "                                          data_dict[record]['email_address'],\\\n",
    "                                          data_dict[record]['poi'],record),)\n",
    "    \n",
    "def tuple_lookup(val,col):\n",
    "    #print val\n",
    "    for id in range(len(email_lookup_tup)):\n",
    "        if email_lookup_tup[id][0] == val:\n",
    "            if col == \"email_address\":\n",
    "                return email_lookup_tup[id][1]\n",
    "            elif col == \"poi\":\n",
    "                return email_lookup_tup[id][2]\n",
    "            elif col == \"name\":\n",
    "                return email_lookup_tup[id][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, the source data is split as a column vector comprising of the person-of-interest (target) and income and email communication summary as a feature vector. In the next step, the feature set is transformed into a pandas dataframe to facilitate introduction of two new synthetic variables. The two new variables are the number of emails sent by a person as a ratio of the total number of emails sent and the number of emails received as a ratio of the total number of emails received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(data_dict, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "people_features = []\n",
    "for i in range(len(labels)):\n",
    "    people_features.append(features[i])\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "df = pd.DataFrame(data=people_features,columns=features_list[1:])\n",
    "df['target'] = pd.Series(np.asarray(labels),index=df.index)\n",
    "df['person_to_poi_proportion'] = df.apply(lambda row: row['from_this_person_to_poi']/\\\n",
    "                                          row['to_messages'] if row['to_messages'] > 0 \\\n",
    "                                          else 0.0, axis=1)\n",
    "df['poi_to_person_proportion'] = df.apply(lambda row: row['from_poi_to_this_person']/\\\n",
    "                                          row['from_messages'] if row['from_messages'] > 0 \\\n",
    "                                          else 0.0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, I performed an independent two-sample t-test for each of the variables in the dataset. The main purpose of this exercise was to identify and exclude variables that did not appear to have a statistically significant effect on the mean between the two groups (poi = 0 and poi = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group1 = df[df['target']==0]\n",
    "group2 = df[df['target']==1]\n",
    "\n",
    "stat_significant_cols = []\n",
    "\n",
    "for var in list(df.columns.values):\n",
    "    if var != \"target\":\n",
    "        if ttest_ind(group1[var], group2[var])[1] <= 0.05:\n",
    "            stat_significant_cols.append(var)\n",
    "\n",
    "new_list_of_features = [\"poi\"]\n",
    "calculated_features = []\n",
    "for var in stat_significant_cols[:]:\n",
    "    if var in features_list and var not in new_list_of_features:\n",
    "        new_list_of_features.append(var)\n",
    "    elif var == \"person_to_poi_proportion\":\n",
    "        if 'from_this_person_to_poi' not in new_list_of_features:\n",
    "            new_list_of_features.append('from_this_person_to_poi')\n",
    "        if 'to_messages' not in new_list_of_features:\n",
    "            new_list_of_features.append('to_messages')\n",
    "    elif var == \"poi_to_person_proportion\":\n",
    "        if 'from_poi_to_this_person' not in new_list_of_features:\n",
    "            new_list_of_features.append('from_poi_to_this_person')\n",
    "        if 'from_messages' not in new_list_of_features:\n",
    "            new_list_of_features.append('from_messages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we rebuild the list of features based on the results from our independent sample t-tests. As part of this exercise, I also chose to drop two of the income variables included in our original dataset - \"total_payments\" and \"total_stock_value\". These two variables are the sum totals of others. Hence, knowing all the other variables, one can determine these two variables and therefore they don't add any value.\n",
    "In the next step, we perform principal component analysis on the income variables with the intent of reducing the variables. Results showed that including two components can account for almost 96% of the variability in the income variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>bonus</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>expenses</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <td>0.616</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.725</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus</th>\n",
       "      <td>0.742</td>\n",
       "      <td>0.639</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.563</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restricted_stock</th>\n",
       "      <td>0.559</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.539</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expenses</th>\n",
       "      <td>0.345</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.186</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_advances</th>\n",
       "      <td>0.420</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.130</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferred_income</th>\n",
       "      <td>-0.367</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_term_incentive</th>\n",
       "      <td>0.557</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.435</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>0.591</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>0.587</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         salary  exercised_stock_options  bonus  \\\n",
       "salary                    1.000                    0.616  0.742   \n",
       "exercised_stock_options   0.616                    1.000  0.639   \n",
       "bonus                     0.742                    0.639  1.000   \n",
       "restricted_stock          0.559                    0.750  0.539   \n",
       "expenses                  0.345                    0.106  0.250   \n",
       "loan_advances             0.420                    0.725  0.563   \n",
       "deferred_income          -0.367                   -0.220 -0.340   \n",
       "long_term_incentive       0.557                    0.480  0.459   \n",
       "other                     0.591                    0.720  0.520   \n",
       "\n",
       "                         restricted_stock  expenses  loan_advances  \\\n",
       "salary                              0.559     0.345          0.420   \n",
       "exercised_stock_options             0.750     0.106          0.725   \n",
       "bonus                               0.539     0.250          0.563   \n",
       "restricted_stock                    1.000     0.186          0.606   \n",
       "expenses                            0.186     1.000          0.130   \n",
       "loan_advances                       0.606     0.130          1.000   \n",
       "deferred_income                    -0.134    -0.070         -0.035   \n",
       "long_term_incentive                 0.349     0.076          0.435   \n",
       "other                               0.656     0.151          0.770   \n",
       "\n",
       "                         deferred_income  long_term_incentive  other  \n",
       "salary                            -0.367                0.557  0.591  \n",
       "exercised_stock_options           -0.220                0.480  0.720  \n",
       "bonus                             -0.340                0.459  0.520  \n",
       "restricted_stock                  -0.134                0.349  0.656  \n",
       "expenses                          -0.070                0.076  0.151  \n",
       "loan_advances                     -0.035                0.435  0.770  \n",
       "deferred_income                    1.000               -0.279 -0.358  \n",
       "long_term_incentive               -0.279                1.000  0.587  \n",
       "other                             -0.358                0.587  1.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_list_of_features.remove('total_payments')\n",
    "new_list_of_features.remove('total_stock_value')\n",
    "income_variables = [\"salary\",\"exercised_stock_options\",\\\n",
    "                    \"bonus\",\"restricted_stock\",\\\n",
    "                    \"expenses\",\"loan_advances\",\"deferred_income\",\\\n",
    "                    \"long_term_incentive\",\"other\"]\n",
    "\n",
    "for var in income_variables:\n",
    "    if var in new_list_of_features:\n",
    "        new_list_of_features.remove(var)\n",
    "    new_list_of_features.append(var)\n",
    "new_list_of_features.append('person_id')\n",
    "\n",
    "features_list = new_list_of_features[:]\n",
    "#print features_list\n",
    "data = featureFormat(data_dict, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "test_correlation = pd.DataFrame(data=np.array(features_train)[:,5:-1],columns=income_variables)\n",
    "display(test_correlation.corr(method='pearson'))\n",
    "\n",
    "pca = RandomizedPCA(n_components=len(income_variables)).fit(np.array(features_train)[:,5:-1])\n",
    "total_variance_captured = 0\n",
    "transformed_features = []\n",
    "eigenvectors = []\n",
    "for indexval in range(len(pca.explained_variance_ratio_)):\n",
    "    if total_variance_captured < 0.95:\n",
    "        total_variance_captured += pca.explained_variance_ratio_[indexval]\n",
    "    else:\n",
    "        break\n",
    "#print indexval, total_variance_captured\n",
    "mod_pca = RandomizedPCA(n_components=indexval).fit(np.array(features_train)[:,5:-1])\n",
    "final_features = mod_pca.transform(np.array(features_train)[:,5:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, we concatenate the transformed income features with the number of emails sent by a person as a ratio of the total number of emails sent and the number of emails received as a ratio of the total number of emails received. Lastly, we add a synthetic variable, person_id, which will serve as a pointer to the original data and facilitate the process of pooling the results from the Enron corpus with the income information for each of the records in our dataset. As a final step in the data preparation process, the raw features were normalized such that the sample mean equates to 0 and the standard deviation to 1. The same transformation was applied to the test data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_features = np.concatenate((final_features, np.array(features_train)[:,0:5]), axis=1)\n",
    "final_features = np.concatenate((final_features, np.array(features_train)[:,-1:]), axis=1)\n",
    "\n",
    "final_features[:,3] = final_features[:,3]/final_features[:,6]\n",
    "final_features[:,4] = final_features[:,4]/final_features[:,5]\n",
    "## Correct this\n",
    "final_features[:,3] = np.nan_to_num(final_features[:,3])\n",
    "final_features[:,4] = np.nan_to_num(final_features[:,4])\n",
    "final_features = np.delete(final_features, [5,6], 1)\n",
    "\n",
    "final_features[:,0] = (final_features[:,0] - np.mean(final_features[:,0]))/\\\n",
    "np.std(final_features[:,0])\n",
    "final_features[:,1] = (final_features[:,1] - np.mean(final_features[:,1]))/\\\n",
    "np.std(final_features[:,1])\n",
    "final_features[:,2] = (final_features[:,2] - np.mean(final_features[:,2]))/\\\n",
    "np.std(final_features[:,2])\n",
    "final_features[:,3] = (final_features[:,3] - np.mean(final_features[:,3]))/\\\n",
    "np.std(final_features[:,3])\n",
    "final_features[:,4] = (final_features[:,4] - np.mean(final_features[:,4]))/\\\n",
    "np.std(final_features[:,4])\n",
    "\n",
    "#print final_features[0]\n",
    "final_features_test = mod_pca.transform(np.array(features_test)[:,5:-1])\n",
    "final_features_test = np.concatenate((final_features_test, np.array(features_test)[:,0:5]), axis=1)\n",
    "final_features_test = np.concatenate((final_features_test, np.array(features_test)[:,-1:]), axis=1)\n",
    "final_features_test[:,3] = final_features_test[:,3]/final_features_test[:,6]\n",
    "final_features_test[:,4] = final_features_test[:,4]/final_features_test[:,5]\n",
    "final_features_test[:,3] = np.nan_to_num(final_features_test[:,3])\n",
    "final_features_test[:,4] = np.nan_to_num(final_features_test[:,4])\n",
    "final_features_test = np.delete(final_features_test, [5,6], 1)\n",
    "\n",
    "final_features_test[:,0] = (final_features_test[:,0] - np.mean(final_features_test[:,0]))/\\\n",
    "np.std(final_features_test[:,0])\n",
    "final_features_test[:,1] = (final_features_test[:,1] - np.mean(final_features_test[:,1]))/\\\n",
    "np.std(final_features_test[:,1])\n",
    "final_features_test[:,2] = (final_features_test[:,2] - np.mean(final_features_test[:,2]))/\\\n",
    "np.std(final_features_test[:,2])\n",
    "final_features_test[:,3] = (final_features_test[:,3] - np.mean(final_features_test[:,3]))/\\\n",
    "np.std(final_features_test[:,3])\n",
    "final_features_test[:,4] = (final_features_test[:,4] - np.mean(final_features_test[:,4]))/\\\n",
    "np.std(final_features_test[:,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my first analysis, I tried a logit specification with \"poi\" as the response variable with the income and the email variables as the regressors and used the complexity factor as the only hyperparameter. The results hsown below indicate that the logit specification has a reasonably well precision and recall. Unfortunately, the model and regressor significance cannot be obtained with the sklearn logistic regression implementation. Hence, I used the same specification with the logistic regression implementation available in the statmodels library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37  1]\n",
      " [ 3  2]]\n"
     ]
    }
   ],
   "source": [
    "param_grid_logit = {\n",
    "         'C': [100000.0, 10000.0, 1000.0, 100.0, 10.0, 1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001],\n",
    "         'class_weight': [\"balanced\",None]\n",
    "          }\n",
    "\n",
    "clf = GridSearchCV(linear_model.LogisticRegression(), param_grid_logit)\n",
    "clf = clf.fit(final_features[:,[0,1,2,3,4]], labels_train)\n",
    "logit_model = clf.best_estimator_\n",
    "\n",
    "predictions_logit = logit_model.predict(final_features_test[:,[0,1,2,3,4]])\n",
    "c_matrix_test = confusion_matrix(labels_test, predictions_logit[:])\n",
    "print c_matrix_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regressor significances shown below indicate that the two synthetic variables - poi_to_person_proportion and person_to_poi_proportion are almost insignificant. Hence, I decided to revise the model after dropping these two regressors from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -0.18189243  -0.42137307  -0.62578358  -0.40237325  -0.42637617  16.        ]\n",
      "('Parameters: ', Intercept                   -2.131\n",
      "finance1_n                   0.575\n",
      "finance2_n                   0.241\n",
      "shared_receipt_with_poi_n    0.606\n",
      "poi_to_person_proportion     0.016\n",
      "person_to_poi_proportion    -0.063\n",
      "dtype: float64)\n",
      "        Logit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:                      y\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "=============================================================================================\n",
      "                               dy/dx    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "---------------------------------------------------------------------------------------------\n",
      "finance1_n                    0.0538      0.070      0.764      0.445        -0.084     0.192\n",
      "finance2_n                    0.0226      0.026      0.881      0.378        -0.028     0.073\n",
      "shared_receipt_with_poi_n     0.0567      0.024      2.399      0.016         0.010     0.103\n",
      "poi_to_person_proportion      0.0015      0.026      0.059      0.953        -0.049     0.052\n",
      "person_to_poi_proportion     -0.0059      0.037     -0.159      0.873        -0.078     0.066\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "print final_features[0]\n",
    "final_features_logit = [\"finance1_n\",\n",
    "                        \"finance2_n\",\n",
    "                        \"shared_receipt_with_poi_n\",\n",
    "                        \"poi_to_person_proportion\",\n",
    "                        \"person_to_poi_proportion\",\n",
    "                        \"person_id\"]\n",
    "final_features_df = pd.DataFrame(final_features,columns = final_features_logit)\n",
    "\n",
    "final_features_df['Response'] = final_features_df['person_id'].\\\n",
    "apply(lambda x: tuple_lookup(x,col=\"poi\"))\n",
    "\n",
    "y, X = dmatrices('Response ~ finance1_n + finance2_n + shared_receipt_with_poi_n + poi_to_person_proportion\\\n",
    "                + person_to_poi_proportion',\\\n",
    "                 final_features_df, return_type=\"dataframe\")\n",
    "\n",
    "y_flat = np.ravel(y['Response[True]'])\n",
    "model = sm.Logit(y_flat, X)\n",
    "model_summary = model.fit(disp=0)\n",
    "print('Parameters: ', model_summary.params)\n",
    "\n",
    "model_significance = model_summary.get_margeff()\n",
    "print(model_significance.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I re-ran the model with the variables that were relatively significant but there was little improvement in the the significance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Parameters: ', Intercept                   -2.131\n",
      "finance1_n                   0.585\n",
      "finance2_n                   0.246\n",
      "shared_receipt_with_poi_n    0.603\n",
      "dtype: float64)\n",
      "        Logit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:                      y\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "=============================================================================================\n",
      "                               dy/dx    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "---------------------------------------------------------------------------------------------\n",
      "finance1_n                    0.0548      0.071      0.766      0.443        -0.085     0.195\n",
      "finance2_n                    0.0230      0.026      0.897      0.370        -0.027     0.073\n",
      "shared_receipt_with_poi_n     0.0565      0.023      2.470      0.013         0.012     0.101\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "y, X = dmatrices('Response ~ finance1_n + finance2_n + shared_receipt_with_poi_n',\\\n",
    "                 final_features_df, return_type=\"dataframe\")\n",
    "\n",
    "y_flat = np.ravel(y['Response[True]'])\n",
    "model = sm.Logit(y_flat, X)\n",
    "model_summary = model.fit(disp=0)\n",
    "print('Parameters: ', model_summary.params)\n",
    "\n",
    "model_significance = model_summary.get_margeff()\n",
    "print(model_significance.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this, I obtained a revised confusion matrix based using only three regressors - the two income variables and the number of emails shared with poi-s. As expected, there was no improvement as evident from the True Positive, False Positive, and False Negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37  1]\n",
      " [ 3  2]]\n"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(linear_model.LogisticRegression(), param_grid_logit)\n",
    "clf = clf.fit(final_features[:,[0,1,2]], labels_train)\n",
    "logit_model_2 = clf.best_estimator_\n",
    "\n",
    "predictions_logit_2 = logit_model_2.predict(final_features_test[:,[0,1,2]])\n",
    "c_matrix_test_2 = confusion_matrix(labels_test, predictions_logit_2[:])\n",
    "print c_matrix_test_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final revision, I added two interaction features. I ran a few other specifications as well and retained this one based on the statistical significance of the individual regressors. One noticeable observation with this specification was that the significance of the \"shared_receipt_with_poi\" dropped considerably and this was mostly due to the inclusion of the two interaction factors - \"finance_1\"/\"shared_receipt_with_poi\" and \"finance_2\"/\"shared_receipt_with_poi\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Parameters: ', Intercept                                0.415\n",
      "finance1_n                              24.755\n",
      "finance2_n                              -3.928\n",
      "shared_receipt_with_poi_n               -0.964\n",
      "finance1_n:shared_receipt_with_poi_n   -15.588\n",
      "finance2_n:shared_receipt_with_poi_n     2.750\n",
      "dtype: float64)\n",
      "        Logit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:                      y\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "========================================================================================================\n",
      "                                          dy/dx    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "finance1_n                               2.2758      2.198      1.035      0.301        -2.033     6.585\n",
      "finance2_n                              -0.3611      0.386     -0.936      0.349        -1.117     0.395\n",
      "shared_receipt_with_poi_n               -0.0886      0.147     -0.604      0.546        -0.376     0.199\n",
      "finance1_n:shared_receipt_with_poi_n    -1.4331      1.417     -1.012      0.312        -4.209     1.343\n",
      "finance2_n:shared_receipt_with_poi_n     0.2528      0.258      0.980      0.327        -0.253     0.759\n",
      "========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "y, X = dmatrices('Response ~ finance1_n + finance2_n + shared_receipt_with_poi_n \\\n",
    "                        + finance1_n*shared_receipt_with_poi_n + finance2_n*shared_receipt_with_poi_n',\\\n",
    "                 final_features_df, return_type=\"dataframe\")\n",
    "\n",
    "y_flat = np.ravel(y['Response[True]'])\n",
    "model = sm.Logit(y_flat, X)\n",
    "model_summary = model.fit(disp=0)\n",
    "print('Parameters: ', model_summary.params)\n",
    "\n",
    "model_significance = model_summary.get_margeff()\n",
    "print(model_significance.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I re-ran the sklearn implementation of the logistric regression classifier with the additional interaction features and there were no improvement with respect to the previous instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37  1]\n",
      " [ 3  2]]\n"
     ]
    }
   ],
   "source": [
    "final_features_logit_int = np.concatenate((final_features[:,[0,1,2,5]],final_features[:,0:1]*final_features[:,2:3],\\\n",
    "                                           final_features[:,1:2]*final_features[:,2:3])\\\n",
    "                                          ,axis=1)\n",
    "\n",
    "final_features_test_logit_int = np.concatenate((final_features_test[:,[0,1,2,5]],\\\n",
    "                                                final_features_test[:,0:1]*final_features_test[:,2:3],\\\n",
    "                                               final_features_test[:,1:2]*final_features_test[:,2:3]),axis=1)\n",
    "\n",
    "clf = GridSearchCV(linear_model.LogisticRegression(), param_grid_logit)\n",
    "clf = clf.fit(final_features[:,[0,1,2,4,5]], labels_train)\n",
    "logit_model_3 = clf.best_estimator_\n",
    "\n",
    "predictions_logit_3 = logit_model_3.predict(final_features_test[:,[0,1,2,4,5]])\n",
    "c_matrix_test_3 = confusion_matrix(labels_test, predictions_logit_3[:])\n",
    "print c_matrix_test_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my next algorithm, I chose support vector classifier (SVC). I used the complexity factor, class-weight, kernel, and the degree for polynomial kernel as part of the hyperparameter optimization process to find the optimal SVC classifier. Unfortunately, I was unable to identify not even one of the \"poi\" records and therefore obtained zero for both precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38  0]\n",
      " [ 5  0]]\n"
     ]
    }
   ],
   "source": [
    "param_grid_svc = {\n",
    "         'C': [100000.0, 10000.0, 1000.0, 100.0, 10.0, 1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001],\n",
    "         'class_weight': [\"balanced\",None],\n",
    "         'kernel': [\"linear\", \"poly\", \"sigmoid\", \"rbf\"],\n",
    "         'degree': [2,3,4,5]\n",
    "          }\n",
    "clf = GridSearchCV(SVC(), param_grid_svc)\n",
    "clf = clf.fit(final_features[:,[0,1,2,3,4]], labels_train)\n",
    "svm_model = clf.best_estimator_\n",
    "\n",
    "predictions_svm = svm_model.predict(final_features_test[:,[0,1,2,3,4]])\n",
    "c_matrix_test_svm = confusion_matrix(labels_test, predictions_svm[:])\n",
    "print c_matrix_test_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees were my next choice for classification algorithms. I used the evaluation criteria, depth of the tree, class-weight, minimum number of samples required for splitting, and minimum number of samples per leaf in my hyperparameter optimization step. Given the extremely small number of poi-s (13) in the dataset, I hoped the best paramters for minimum number of samples per leaf would tend to be extremely low but results suggested otherwise. However, this translated to non-identification of any of the poi-s in the dataset. Like the logistic regression results, the decision tree algorithm indicated the number of emails shared with poi as the most important feature for identifying the poi-s in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 2, 'min_samples_leaf': 5, 'criterion': 'gini', 'max_depth': 3, 'class_weight': None}\n",
      "[[36  2]\n",
      " [ 5  0]]\n",
      "[ 0.02493101  0.          0.39967268  0.31801683  0.25737948]\n"
     ]
    }
   ],
   "source": [
    "param_grid_dtree = {\n",
    "         'criterion':['gini','entropy'],\n",
    "         'max_depth':[2,3,4,5,6],\n",
    "         'class_weight': [\"balanced\",None],\n",
    "         'min_samples_split':[2,4,6,8],\n",
    "         'min_samples_leaf':[1,2,3,4,5,6,7]\n",
    "          }\n",
    "clf = GridSearchCV(tree.DecisionTreeClassifier(random_state = 42), param_grid_dtree)\n",
    "clf = clf.fit(final_features[:,[0,1,2,3,4]], labels_train)\n",
    "dtree_model = clf.best_estimator_\n",
    "print clf.best_params_\n",
    "\n",
    "predictions_dtree = dtree_model.predict(final_features_test[:,[0,1,2,3,4]])\n",
    "c_matrix_test_dtree = confusion_matrix(labels_test, predictions_dtree[:])\n",
    "print c_matrix_test_dtree\n",
    "print dtree_model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a revision to the baseline decision tree model developed in the previous step, I considered including only the two most important features (based on the gini information criterion) in the dataset. The improvement in the results were remarkable as we were able to identify two of the five poi-s in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini', 'max_depth': 2, 'class_weight': 'balanced'}\n",
      "[[33  5]\n",
      " [ 3  2]]\n",
      "[ 0.72112496  0.27887504]\n"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(tree.DecisionTreeClassifier(random_state = 42), param_grid_dtree)\n",
    "clf = clf.fit(final_features[:,[2,3]], labels_train)\n",
    "dtree_model_2 = clf.best_estimator_\n",
    "print clf.best_params_\n",
    "\n",
    "predictions_dtree_2 = dtree_model_2.predict(final_features_test[:,[2,3]])\n",
    "c_matrix_test_dtree_2 = confusion_matrix(labels_test, predictions_dtree_2[:])\n",
    "print c_matrix_test_dtree_2\n",
    "print dtree_model_2.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My next classification algorithm of choice was Naive Bayes. Considering the simplicity of the algorithm, the results were fairly promising. The confusion matrix shown below indicate that the precision and recall were both 0.4 with Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35  3]\n",
      " [ 3  2]]\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "nb_model = gnb.fit(final_features[:,[0,1,2,3,4]], labels_train)\n",
    "\n",
    "test_predict_nb = nb_model.predict(final_features_test[:,[0,1,2,3,4]])\n",
    "print confusion_matrix(labels_test, test_predict_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My final classification algorithm of choice included Random Forests (RF). Random forest is an ensemble technique that builds the final algorithm from an ensemble of numerous weak classifiers through majority voting or mean prediction (in case of continuous response variable). Ensemble methods typically tend to be more robust and thus outperform other classification algorithms for datasets that are noisy (like the Enron dataset) and therefore I was more optimistic with the Random Forest classifier. Unfortunately, the results below indicate that the RF model fails to identify the poi-s in the dataset. However, we noticed that the emails shared with poi-s were once again the most important feature in the datset for identifying potential poi-s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 2, 'n_estimators': 500, 'criterion': 'gini', 'min_samples_split': 6, 'max_depth': 4, 'class_weight': 'balanced'}\n",
      "[[36  2]\n",
      " [ 5  0]]\n",
      "[ 0.218844    0.16692288  0.27486857  0.19957543  0.13978913]\n"
     ]
    }
   ],
   "source": [
    "param_grid_RF = {\n",
    "         'criterion':['gini','entropy'],\n",
    "         'n_estimators':[100,250,500],\n",
    "         'class_weight': [\"balanced\",None],\n",
    "         'min_samples_split':[2,4,6,8],\n",
    "         'min_samples_leaf':[1,2,3,4,5],\n",
    "         'max_depth':[4,6,8,10]\n",
    "          }\n",
    "clf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_RF)\n",
    "clf = clf.fit(final_features[:,[0,1,2,3,4]], labels_train)\n",
    "RF_model = clf.best_estimator_\n",
    "print clf.best_params_\n",
    "\n",
    "predictions_RF = RF_model.predict(final_features_test[:,[0,1,2,3,4]])\n",
    "c_matrix_test_RF = confusion_matrix(labels_test, predictions_RF[:])\n",
    "print c_matrix_test_RF\n",
    "print RF_model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a revision to the RF model developed in the previous step, I considered excluding the two variables that exhibited the lowest importance of all the features in the dataset. Results indicate that tha revised model has a 0.4 precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 4, 'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 4, 'class_weight': 'balanced'}\n",
      "[[35  3]\n",
      " [ 3  2]]\n",
      "[ 0.38267731  0.3524988   0.26482388]\n"
     ]
    }
   ],
   "source": [
    "param_grid_RF_2 = {\n",
    "         'criterion':['gini','entropy'],\n",
    "         'n_estimators':[100,250,500],\n",
    "         'class_weight': [\"balanced\",None],\n",
    "         'min_samples_split':[2,4,6,8],\n",
    "         'min_samples_leaf':[1,2,3,4,5],\n",
    "         'max_depth':[4,6,8,10]\n",
    "          }\n",
    "clf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_RF_2)\n",
    "clf = clf.fit(final_features[:,[0,2,3]], labels_train)\n",
    "RF_model_2 = clf.best_estimator_\n",
    "print clf.best_params_\n",
    "\n",
    "predictions_RF_2 = RF_model_2.predict(final_features_test[:,[0,2,3]])\n",
    "c_matrix_test_RF_2 = confusion_matrix(labels_test, predictions_RF_2[:])\n",
    "print c_matrix_test_RF_2\n",
    "print RF_model_2.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried a further revision based on the feature importances from the previous step but noticed that it failed to identify any of the poi-s in our dataset and hence, I decided to keep the previous RF model as the final RF classifier for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 5, 'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 4, 'class_weight': None}\n",
      "[[38  0]\n",
      " [ 5  0]]\n",
      "[ 0.4600568  0.5399432]\n"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_RF_2)\n",
    "clf = clf.fit(final_features[:,[0,2]], labels_train)\n",
    "RF_model_3 = clf.best_estimator_\n",
    "print clf.best_params_\n",
    "\n",
    "predictions_RF_3 = RF_model_3.predict(final_features_test[:,[0,2]])\n",
    "c_matrix_test_RF_3 = confusion_matrix(labels_test, predictions_RF_3[:])\n",
    "print c_matrix_test_RF_3\n",
    "print RF_model_3.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, I tried to look at the email_texts for each person inthe dataset with the objective of finding additional information that would help us improve the precison and recall further. To that effect, my approach was o identify key words in the emails that can be associated with the persons of interest. I used a two-step approach to achieve this objective. In the first step, I tried to identify top 10 words that best identify emails that originated from the poi-s in the training set and in the second step I tried combining these words with the income and email summary features using a decision tree classifier.\n",
    "As a first step in this process, I started off with loading existing user-defined helper functions to load the email data from the Enron corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/ambarishbanerjee/Downloads/ud120-projects/tools/\")\n",
    "from parse_out_email_text import parseOutText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I start by concatenating all emails for each person in our dataset into a single string and filtering (this was done iteratively) out proper nouns as they provide no information for poi-s in general but rather bear a signature for the person who authored the email. Finally, I include the person_id and the poi-variable to index and keep track of the response variable for each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/ambarishbanerjee/Downloads/ud120-projects/final_project/emails_by_address/\")\n",
    "#print os.getcwd()\n",
    "email_text_data = {}\n",
    "all_person_email = []\n",
    "\n",
    "for email_add in email_list[:]:\n",
    "    email_filename = \"from_\"+email_add[0]+\".txt\"\n",
    "    email_text_data[\"email_id\"] = email_add[0]\n",
    "    email_text_data[\"poi\"] = email_add[1]\n",
    "    email_text_data[\"person_id\"] = email_add[2]\n",
    "    if os.path.isfile(email_filename):\n",
    "        email_text_data[\"email_text\"] = open(email_filename, \"r\")\n",
    "    else:\n",
    "        email_text_data[\"email_text\"] = None\n",
    "    all_person_email.append(email_text_data.copy())\n",
    "    \n",
    "features_train_tm_list = []\n",
    "labels_train_tm_list = []\n",
    "features_test_tm_list = []\n",
    "labels_test_tm_list = []\n",
    "\n",
    "remove_words = ['[A-Za-z]*houect[A-Za-z]*','[A-Za-z]*kamins[A-Za-z]*','[A-Za-z]*keann[A-Za-z]*',\\\n",
    "               '[A-Za-z]*shirley[A-Za-z]*','[A-Za-z]*sbeck[A-Za-z]*','[A-Za-z]*stinson[A-Za-z]*',\\\n",
    "               '[A-Za-z]*shapiro[A-Za-z]*','[A-Za-z]*mcconn[A-Za-z]*','[A-Za-z]*zimin[A-Za-z]*',\\\n",
    "               '[A-Za-z]*jlavoransf[A-Za-z]*','[A-Za-z]*jshankmnsf[A-Za-z]*','[A-Za-z]*mhaedicnsf[A-Za-z]*',\n",
    "               '[A-Za-z]*maureen[A-Za-z]*','[A-Za-z]*pallennsf[A-Za-z]*']\n",
    "\n",
    "for record in all_person_email[:]:\n",
    "    \n",
    "    all_emails_concat = \"\"\n",
    "    #print record['email_text']\n",
    "    if record['email_text'] != None:\n",
    "        for path in record['email_text']:\n",
    "            ### only look at first 200 emails when developing\n",
    "            ### once everything is working, remove this line to run over full dataset\n",
    "            #print path[path.find('/'):]\n",
    "            path = os.path.join('../..', path[path.find('/')+1:][:-1])\n",
    "            email = open(path, \"r\")\n",
    "            all_emails_concat += parseOutText(email) \n",
    "            email.close()\n",
    "    for word in remove_words:\n",
    "        word_pattern = re.compile(word)\n",
    "        all_emails_concat = re.sub(word_pattern,\"\",all_emails_concat)\n",
    "        \n",
    "    if record[\"person_id\"] in final_features[:,5]:\n",
    "        features_train_tm_list.append(all_emails_concat)\n",
    "        #from_data_train_tm.append(record['email_id'])\n",
    "        labels_train_tm_list.append(record['poi'])\n",
    "    else:\n",
    "        features_test_tm_list.append(all_emails_concat)\n",
    "        #from_data_test_tm.append(record['email_id'])\n",
    "        labels_test_tm_list.append(record['poi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, I vectorized the data using Sklearn TFIdf-vectorizer and used a decision tree classifier on the vectorized data. As a quick note, I tried a few different values for the number of features and I observed that 10 features helped achieve the most number of positive ids with the pois on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "[[86  1]\n",
      " [ 9  4]]\n",
      "[u'brent', u'deriv', u'engin', u'enrononlin', u'ferc', u'hotel', u'lng', u'shall', u'sincer', u'student']\n"
     ]
    }
   ],
   "source": [
    "features_train_tm = np.asarray(features_train_tm_list)\n",
    "features_test_tm = np.asarray(features_test_tm_list)\n",
    "labels_train_tm = np.asarray(labels_train_tm_list)\n",
    "labels_test_tm = np.asarray(labels_test_tm_list)\n",
    "#print features_train_tm[0]\n",
    "\n",
    "param_grid_tm = {\n",
    "         'criterion':['gini','entropy'],\n",
    "         'max_depth':[2,3,4,5],\n",
    "         'class_weight': [\"balanced\",None],\n",
    "         'min_samples_split':[2,4,6,8],\n",
    "         'min_samples_leaf':[1,2,3,4,5,6,7]\n",
    "          }\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",max_df=0.2,max_features=10)\n",
    "features_train_tm = vectorizer.fit_transform(features_train_tm).toarray()\n",
    "features_test_tm  = vectorizer.transform(features_test_tm).toarray()\n",
    "\n",
    "clf = GridSearchCV(tree.DecisionTreeClassifier(random_state=42), param_grid_tm)\n",
    "clf = clf.fit(features_train_tm, labels_train_tm)\n",
    "tm_dtree = clf.best_estimator_\n",
    "\n",
    "pred_train = tm_dtree.predict(features_train_tm)\n",
    "print accuracy_score(labels_train_tm,pred_train)\n",
    "print confusion_matrix(labels_train_tm,pred_train)\n",
    "\n",
    "feature_words = vectorizer.get_feature_names()\n",
    "print feature_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing on the test dataset, I observed that the algorithm failed to identify even one of the pois in the test set which led me to conclude that text mining approaches may not be suitable for identifying the poi-s in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.767441860465\n",
      "[[33  5]\n",
      " [ 5  0]]\n"
     ]
    }
   ],
   "source": [
    "pred_test = tm_dtree.predict(features_test_tm)\n",
    "print accuracy_score(labels_test_tm,pred_test)\n",
    "print confusion_matrix(labels_test_tm, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final section of this project, we use the helper functions provided to test the accuracy, recall, precision, and the F1 score. We start by importing the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/ambarishbanerjee/Downloads/ud120-projects/final_project/\")\n",
    "from tester import test_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we combine the training and test data used thus far to obtain the full list of transformed features that were generated prior to model development. Following this step, these synthetic features were merged in the original dataset as required by the framework for this project. The additional features are also included in the final feature set for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined_data = np.concatenate((final_features,final_features_test),axis=0)\n",
    "for record in combined_data[:,:]:\n",
    "    #print record[-1]\n",
    "    data_dict[tuple_lookup(record[-1],\"name\")][\"finance_1_n\"] = record[0]\n",
    "    data_dict[tuple_lookup(record[-1],\"name\")][\"finance_2_n\"] = record[1]\n",
    "    data_dict[tuple_lookup(record[-1],\"name\")][\"shared_with_poi_n\"] = record[2]\n",
    "    data_dict[tuple_lookup(record[-1],\"name\")][\"from_poi_to_person_ratio_n\"] = record[3]\n",
    "    data_dict[tuple_lookup(record[-1],\"name\")][\"from_person_to_poi_ratio_n\"] = record[4]\n",
    "\n",
    "if \"finance_1_n\" not in features_list:\n",
    "    features_list.extend([\"finance_1_n\",\"finance_2_n\",\"shared_with_poi_n\",\\\n",
    "                         \"from_poi_to_person_ratio_n\",\"from_person_to_poi_ratio_n\"])\n",
    "#print features_list[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our first algorithm, we evaluate the logistic regression specification with the two transformed income variables and the number of emails shared with persons-of-interest as our regressors. Please note that certain other regressors were dropped from the model progressively due to low statistical significance. The results show us that the precision and recall for this model are 0.57 and 0.2, respectively. This implies that the model is successful 1 in 5 times in identifying the poi-s given that the person is actually a poi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.87333\tPrecision: 0.57143\tRecall: 0.20000\tF1: 0.29630\tF2: 0.22989\n",
      "\tTotal predictions: 1500\tTrue positives:   40\tFalse positives:   30\tFalse negatives:  160\tTrue negatives: 1270\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_list_logit = [\"poi\",\"finance_1_n\",\"finance_2_n\",\"shared_with_poi_n\"]\n",
    "test_classifier(logit_model_2, data_dict, features_list_logit, folds = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we evaluated the decision tree model we developed with two of the features from the entire dataset. The first of these includes the number of emails shared between a given person and a poi and the latter being the ratio of the messages received from poi-s to the total of the messages received. Once again, we started with all five engineered features but dropped three of them as their gini-index (indicates the information contained in those features) was relatively low. The results below indicate that the decision tree model is successful in identifying 1 in every 2 poi-s given that they are actually poi-s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='best')\n",
      "\tAccuracy: 0.84067\tPrecision: 0.42412\tRecall: 0.54500\tF1: 0.47702\tF2: 0.51561\n",
      "\tTotal predictions: 1500\tTrue positives:  109\tFalse positives:  148\tFalse negatives:   91\tTrue negatives: 1152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_list_dtree = [\"poi\",\"shared_with_poi_n\",\"from_poi_to_person_ratio_n\"]\n",
    "test_classifier(dtree_model_2, data_dict, features_list_dtree, folds = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our third algorithm for this project is the Naive Bayes classifier. We included all five engineered variables for this model as there was no way to determine individual significance of each of the features in the dataset. The model performed relatively well given its simplicity; results showed that it was able to identify 1 in every 3 poi-s in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "\tAccuracy: 0.85467\tPrecision: 0.43750\tRecall: 0.31500\tF1: 0.36628\tF2: 0.33369\n",
      "\tTotal predictions: 1500\tTrue positives:   63\tFalse positives:   81\tFalse negatives:  137\tTrue negatives: 1219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_list_nb = [\"poi\",\"finance_1_n\",\"finance_2_n\",\"shared_with_poi_n\",\\\n",
    "                         \"from_poi_to_person_ratio_n\",\"from_person_to_poi_ratio_n\"]\n",
    "test_classifier(nb_model, data_dict, features_list_nb, folds = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final classification algorithm that we evaluated was the Random Forest. Once again we limited our feature set to only those that had a relatively high Gini-index. The precision and recall values were higher 0.35 and 0.4, respectively implying that the random forest classifier is able to identify 2 out of every 5 poi-s in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=4, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.82000\tPrecision: 0.34649\tRecall: 0.39500\tF1: 0.36916\tF2: 0.38424\n",
      "\tTotal predictions: 1500\tTrue positives:   79\tFalse positives:  149\tFalse negatives:  121\tTrue negatives: 1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_list_rf = [\"poi\",\"finance_1_n\",\"shared_with_poi_n\",\\\n",
    "                         \"from_poi_to_person_ratio_n\"]\n",
    "test_classifier(RF_model_2, data_dict, features_list_rf, folds = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results presented above, the decision tree model outperformed the other algorithms considered in this project. In order to be consistent and for the purposes of comparison, it can be said that the feature set was varied between the models. Hence in the final section of this project, I revised the logistic regression, naive bayes, and random forest model to obtain the precision, recall and F1 score on a consistent feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the precision, recall, and F1 score for the logit model dropped drastically as we switched from the optimal choice of features to a suboptimal choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.86000\tPrecision: 0.18750\tRecall: 0.01500\tF1: 0.02778\tF2: 0.01838\n",
      "\tTotal predictions: 1500\tTrue positives:    3\tFalse positives:   13\tFalse negatives:  197\tTrue negatives: 1287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(linear_model.LogisticRegression(), param_grid_logit)\n",
    "clf = clf.fit(final_features[:,[2,3]], labels_train)\n",
    "logit_model_comp = clf.best_estimator_\n",
    "\n",
    "test_classifier(logit_model_comp, data_dict, features_list_dtree, folds = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Naive Bayes classifier, we make a similar observation as the logistic classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "\tAccuracy: 0.82333\tPrecision: 0.20183\tRecall: 0.11000\tF1: 0.14239\tF2: 0.12101\n",
      "\tTotal predictions: 1500\tTrue positives:   22\tFalse positives:   87\tFalse negatives:  178\tTrue negatives: 1213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "nb_model_comp = gnb.fit(final_features[:,[2,3]], labels_train)\n",
    "\n",
    "test_classifier(nb_model_comp, data_dict, features_list_dtree, folds = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the random forest based classifier showed a significant improvement in precision, recall, and F1 scores after switching to the new set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=4, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.85333\tPrecision: 0.45614\tRecall: 0.52000\tF1: 0.48598\tF2: 0.50584\n",
      "\tTotal predictions: 1500\tTrue positives:  104\tFalse positives:  124\tFalse negatives:   96\tTrue negatives: 1176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_RF_2)\n",
    "clf = clf.fit(final_features[:,[2,3]], labels_train)\n",
    "RF_model_comp = clf.best_estimator_\n",
    "\n",
    "test_classifier(RF_model_comp, data_dict, features_list_dtree, folds = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the precision and recall values, I chose to keep the decision tree model as it outperformed the other classification algorithms. As required by the framework of this project, I make a final call to one of the helper functions to write the feature and data set to separate files so that one may validate the results and the findings of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = dtree_model_2\n",
    "features_list = features_list_dtree\n",
    "dump_classifier_and_data(dtree_model_2, data_dict, features_list_dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this project, I investigated the Enron fraud data with an objective of developing a classification model that can help us predict the persons of interest given their income information and summary statistics on the emails exchanged. Following are the steps I took and the findings I made as part of analyzing this dataset:\n",
    "1. I started with loading the raw data and removing the outliers from the dataset. This also involved correcting the income information for two of the people in the dataset based on other reliable sources.\n",
    "2. In the next step, I dropped the two artifical features, namely total_payment and total_stock_value as they provided no additional information once all the other features are known.\n",
    "3. In the next step, I undertook a two-sample independent t-test to retain those features whose means were statically different for the two groups - poi-s and non-poi-s.\n",
    "4. In the next step, I performed a principal component analysis with the objective of reducing the income variable to include as many components as necessary to capture 95% of the variability in the income features. This resulted in two eigenvectors as their eigenvalues summaed to approximately 0.96.\n",
    "5. In the fifth step, I included the number of emails shared between a person and a poi and two synthetic features. The first of these was the ratio emails recived by a person from a poi to the total number of emails recieved by that person and the latter was the number of emails sent by a person to a poi to the total number of emails sent.\n",
    "6. Lastly, I chose to standardize all the features so that they each have zero mean and statndard of deviation equalling to 1.\n",
    "7. Following this I evaluated five different classifiers, namely, Logistic Regression, Decision Trees, Support Vector Classifier, Naive bayes, and Random Forests. I began the model development process for each of them with all five engineered variables and gradually reduced the feature set based on statistical significance or the gini-index (except for Naive Bayes and Support vector Classifier). In case of support vector classifer, I was unable to identify even one of the poi-s in the dataset and hence the decision to drop the classifier from further consideration. In addition, I also considered interaction effects as an additional specfication for the Logistic Regression classifier but it did not improve the model's performance.\n",
    "8. Lastly, I hoped to improve the accuracy of the classifiers developed in the previous step by augmenting the information available from the Enron corpus. To that effect, my approach was identify key terms that frequently feature in the emails sent by persons-of-interest in the dataset. Once the frequent terms are identified, they can be combined with the income and email summary features and then use the combined dataset with a classification algorithm to achieve improved accuracy and F1 score. Unfortunately, the key terms identified by the text mining process proved unsuccessful in identifying the persons-of interest in the test dataset.\n",
    "9. In the final section of this project, we evaluate each of algoithms developed in Step 7 using stratified samples. Interestingly, the logistic regression specification developed with the transformed finance features and the number of emails shared with poi-s feel short of 0.3 recall and accuracy though having the highest precision (0.67) and recall (0.4) for the test data used earlier in this study. However, the decision tree, naive bayes, and the random forest specification scored higher than 0.3 precision and recall. The decision tree scored highest in terms of precision and recall. Another vital observation from this project was that the number of emails shared between any person and a poi proved to be the most powerful feature for identifying the poi-s in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the five email-related features were useful during course of this project, calcualting precise values for three of them (email_sent_by_poi_to_person, email_sent_by_person_to_poi, and email_shared_with_poi) would require apriori knowledge of the poi-s in the dataset. Given the objective of this project (to identify the persons of interest), I suspect that the target variable's signature can be found in each of these three variables. This hypothesis was further strengthened when I noticed that the number of emails_shared_with_poi came out as the most powerful feature in the datset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
